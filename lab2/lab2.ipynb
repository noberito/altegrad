{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noberito/altegrad/blob/main/lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsD-LMKT7XMt"
      },
      "source": [
        "<center>\n",
        "<h1>\n",
        "<h1>APM 53674: ALTeGraD</h1>\n",
        "<h2>Lab Session 2: Pretraining and Supervised Finetuning</h2>\n",
        "<h4>Lecture: Prof. Michalis Vazirgiannis<br>\n",
        "Lab: Dr. Hadi Abdine and Yang Zhang</h4>\n",
        "<h5>Tuesday, October 07, 2025</h5>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit <a href='https://forms.gle/9dyaes6dimfvyjwq6' target=\"_blank\">here</a> a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>October 12\n",
        ", 2025 11:59 PM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Instruction Finetuning</b>"
      ],
      "metadata": {
        "id": "z8Z2srf-7VTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this lab, you will learn about fine-tuning large language models (LLMs) for specific tasks.\n",
        "\n",
        "Instruction fine-tuning enables models to follow human instructions effectively by training on high-quality instruction-response pairs.\n",
        "\n",
        "We will implement these techniques using Python and the Hugging Face ecosystem, including transformers and datasets,  By the end of the lab, you will have hands-on experience in adapting LLMs to specific use cases and evaluating their performance.\n",
        "\n",
        "In summary, we will:\n",
        "\n",
        "* Finetune [Qwen2-0.5B](https://huggingface.co/Qwen/Qwen2-0.5B) on a question/answer dataset.\n",
        "\n",
        "* To reduce the required GPU VRAM for the finetuning, we will use [LoRA](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) and [quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes) techniques.\n",
        "\n",
        "* Compare the results before and after instruction tuning.\n",
        "\n",
        "<center>\n",
        "<img src='https://onedrive.live.com/embed?resid=AE69638675180117%21292802&authkey=%21AO_qaECmI1InIyg&width=634&height=556' width=\"500\">\n",
        "\n",
        "\n",
        "LoRA: Low Rank Adapataion. Taken from LoRA original paper\n",
        "\n",
        "<img src='https://onedrive.live.com/embed?resid=AE69638675180117%21292801&authkey=%21AIBM2HNKRF7tzGo&width=1980&height=866' width=\"700\">\n",
        "\n",
        "QLoRA. Taken from QLoRA original paper\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "Tzv7XNDM7Tim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>Finetuning Qwen2.5-0.5B using HuggingFace's Transfromers</b>"
      ],
      "metadata": {
        "id": "xdl5minU7JhQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUCV4V0ONKeJ"
      },
      "source": [
        "\n",
        "In this section, we will fintune [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) - a powerful open-weight family of language models known for a  strong multilingual and reasoning capabilities - on a question answering dataset.\n",
        "\n",
        "Supervised Fine-Tuning (SFT) is a crucial step in adapting pre-trained language models to specific tasks or domains by training them on high-quality instruction-response pairs. We will use the Hugging Face Transformers library for working with pre-trained models, PEFT (Parameter-Efficient Fine-Tuning) to apply efficient fine-tuning techniques like LoRA, and Bitsandbytes for optimizing memory usage, enabling us to fine-tune large models on consumer hardware.\n",
        "\n",
        "A key aspect of fine-tuning conversational models is structuring prompts correctly using chat templates. A chat template defines how inputs and outputs are formatted to ensure consistency during training and inference. In our lab, we will use the following chat template:\n",
        "```\n",
        "<human>: {Question}\n",
        "<assistant>: {Answer}\n",
        "```\n",
        "\n",
        "Such formats helps the model differentiate between user inputs and assistant responses, ensuring better alignment with real-world chat applications.\n",
        "\n",
        "In this section, we will focus on completion-only fine-tuning, meaning we will train the model only on generating the assistant’s response while not learning to generate the prompt. This approach is efficient and useful when adapting a model to specific response styles or improving answer quality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b>Preparing the environment and installing libraries:<b>"
      ],
      "metadata": {
        "id": "_Kk-AEdr65TO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvYPeqtmLTiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0de43ea-e91d-46b9-f248-2e720e73e8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct  7 18:44:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khRdXTxqy9V_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b52623-f08d-422c-9726-fdbf600227be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.6/564.6 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qqq bitsandbytes torch transformers peft accelerate datasets loralib einops trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHHXf0xHUsx9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModelForCausalLM\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b>Loading the model and the tokenizer:</b>"
      ],
      "metadata": {
        "id": "WzHeSpov7CCZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC-Kv8g8MSuW"
      },
      "source": [
        "\n",
        "\n",
        "In this section, we will load the Qwen model while using the BitsAndBytes library for quantization.\n",
        "\n",
        "The Bitsandbytes library is a powerful tool for optimizing large language model (LLM) training and inference by enabling 8-bit and 4-bit quantization, significantly reducing memory usage while maintaining model performance. Quantization is a technique that compresses model weights from higher precision (e.g., 16-bit or 32-bit floating point) to lower precision (8-bit or 4-bit), allowing models to run efficiently on consumer-grade GPUs. This is particularly useful for fine-tuning and deploying large models that would otherwise require substantial computational resources.\n",
        "\n",
        "In Bitsandbytes, key parameters control how quantization is applied:\n",
        "\n",
        "- **nf4 (Normalized Float 4)**: A 4-bit data type designed to better preserve model accuracy by focusing on commonly used weight ranges.\n",
        "- **bnb_4bit_compute_dtype**.\n",
        "- **bnb_4bit_quant_type**: Specifies the quantization method, commonly \"nf4\" or \"fp4\" (floating-point 4-bit).\n",
        "- **load_in_4bit=True**: Enables 4-bit quantization for efficient memory usage.\n",
        "- **load_in_8bit=True**: Enables 8-bit quantization, which offers a trade-off between efficiency and precision.\n",
        "- **bnb_4bit_use_double_quant**.\n",
        "\n",
        "Quantization works by mapping continuous weight values into a smaller discrete range, which reduces the memory footprint of the model while keeping it functionally effective. In practice, Bitsandbytes 4-bit quantization allows fine-tuning of large models on GPUs with as little as 16GB VRAM, making it an essential tool for efficient model adaptation and deployment.\n",
        "\n",
        "In our lab, we will store the model in the VRAM with 4 bits using the 'nf4' quantization method, do the computation using brain float 16 (BF16) and use double quantization.\n",
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 1: </b><br>\n",
        "What is computation dtype in the context of quantization (which can be specified using bnb_4bit_compute_dtype)? What is the importance of double quantization?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "\n",
        "In the context of quantization, computation dtype is the type used for matrix operations on the matrix weights. In other words, you dequantized the stored weights to computation dtype to keep model's accuracy.\n",
        "\n",
        "Double quantization consists in quantizing the quantization constants. <br>\n",
        "\n",
        "When you first perform quantization on model's weights, you store quantization constants (typically in high-precision floats) for each vector of parameters to be able to go back to the original matrix. <br> However storing these quantization constants can still be memory-greedy (especially for large LLMs). That is why we need to perform quantization on them too to keep in memory the less high-precision floats possible while maintaining model's accuracy.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "0U4t0WAB0hJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: </b><br>\n",
        "According to what is described earlier, fill the gap to create our BitsAndBytes configuration.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "fqOLPM9R0qDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6TaXDnRVKDq"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
        "# MODEL_NAME = \"unsloth/Llama-3.2-1B\" # to go further, try llama with unsloth\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTgKyxhJMeEP"
      },
      "source": [
        "#### <b>Configuring LoRA:</b>\n",
        "\n",
        "PEFT (Parameter-Efficient Fine-Tuning) is a library designed to fine-tune large language models (LLMs) efficiently by updating only a small subset of parameters, instead of the entire model. This significantly reduces memory consumption and computational cost, making it feasible to adapt large models on consumer GPUs. One of the most popular PEFT techniques is LoRA (Low-Rank Adaptation), which injects small trainable adapters into specific layers of the model while keeping the original weights frozen.\n",
        "\n",
        "Instead of modifying the large pre-trained weight matrices directly, **LoRA** decomposes weight updates into two smaller matrices of a lower rank. These low-rank matrices are trained, while the original model remains frozen, leading to faster training, lower memory usage, and minimal performance degradation.\n",
        "\n",
        "When applying LoRA using PEFT, several important parameters are used:\n",
        "\n",
        "- **r (Rank)**: The rank of the low-rank matrices added to the model.\n",
        "Common Practice: Values like 8, 16, or 32 are often used. Higher ranks improve model adaptability but require more memory. In our lab we will use a LoRA rank of 32.\n",
        "- **lora_alpha**: The scaling factor for LoRA updates.\n",
        "Common Practice: Set as 2 × rank (e.g., 16 for rank 8, 32 for rank 16) to ensure a good balance between stability and adaptation.\n",
        "- **lora_dropout**: Dropout applied to LoRA layers to prevent overfitting.\n",
        "Common Practice: 0.05–0.1 is commonly used. In our lb we will use 0.05.\n",
        "- **target_modules**: Specifies which model layers should be fine-tuned with LoRA.\n",
        "Common Practice: For transformer models like LLaMA, Qwen, and Mistral, LoRA is typically applied on all projection (MLP) layers inside the transformer block (so excluding the embedding and language modeling head layers).\n",
        "\n",
        " **Note:** set `bias` to `'none'` and do not forget to set the `task_type` to the causla language modeling task.\n",
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: </b><br>\n",
        "Fill the gap in the next cell to compute the number of trainable parameters in a pytorch model in order to check later the effect of using LoRA. <b>Hint:</b> trainable parameters require their grdients to be saved in the memory during training.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIvuxW4lVW_E"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "According to what is described earlier, fill the gap to create your LoRA configuration then use it to define your model. <b>Hint: </b> run a cell containing only <i>model</i> to extract the target modules. <b>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "nywupL1V_Y1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duTYSKKYVamH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e98229-bb3e-4d88-8478-e739d9e01d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 136178560 || all params: 315119488 || trainable%: 43.21489631260127\n",
            "trainable params: 17596416 || all params: 332715904 || trainable%: 5.288721034507566\n"
          ]
        }
      ],
      "source": [
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    target_modules=\"all-linear\",\n",
        "    lora_alpha=64, ## 2 * r\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 2: </b><br>\n",
        "With a small language model of 0.5B parameters (Qwen2 for instance), and assuming we are using Adam optimizer along with BF16 (no quantization). Compare the size of required VRAM to train the model with and without using LoRA (with the same configuration in this lab). Please detail you answer (i.e. required VRAM for model parameters, gradients and optimizer states. <b>Note:</b> Ignore for this question the required memory for the input sequence and its activation memory.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "hG1vTbsx_p5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "\n",
        "<b>Without Lora</b>\n",
        "$$N_{param} = 0.5B$$\n",
        "\n",
        "The number of gradients to store is equal to the number of trainable parameters. <br>\n",
        "$$N_{Standard,grad} = N_{param} = 0.5B$$\n",
        "<br>\n",
        "Adam optimizer is storing the first moment and the second moment per trainable parameter. <br>\n",
        "$$N_{Standard,opt-states} = 2 * N_{param} = 1B$$\n",
        "\n",
        "\n",
        "$$\n",
        "m = (N_{param} + N_{standard,grad} + N_{standard,opt-states}) * BF16_{size}\n",
        "$$\n",
        "$$\n",
        "m_{Standard} = 2B * 2 = 4GB\n",
        "$$\n",
        "\n",
        "<b>With Lora</b>\n",
        "\n",
        "The weight matrix is frozen\n",
        "$$N_{frozen-param} = 0.5B$$\n",
        "The 2 LoRA matrices are trainable and represents 5% of the total params:\n",
        "$$N_{trainable-param} = N_{frozen-param} * 0.05 = 0.025B$$\n",
        "$$N_{Lora,grad} = N_{trainable-param} = 0.025B$$\n",
        "$$N_{Lora,opt-states} = 2 * N_{trainable-param} = 0.05B$$\n",
        "\n",
        "\n",
        "$$\n",
        "m_{Lora} = (N_{frozen-param} + N_{trainable-param}+ N_{Lora,grad} + N_{Lora,opt-states}) * BF16_{size}\n",
        "$$\n",
        "$$\n",
        "m = 0.6B * 2 = 1.2GB\n",
        "$$\n",
        "\n",
        "<b>CONCLUSION</b>\n",
        "\n",
        "Lora reduces the memory used by 70%.\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "qtUCDSN6_6Ah"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H1bBQaSNVsr"
      },
      "source": [
        "#### <b>Test the model before finetuning:</b>\n",
        "\n",
        "A chat template defines how inputs and responses are formatted when interacting with a conversational model. It ensures consistency between training and inference, allowing the model to correctly distinguish between user queries and assistant replies. A well-structured template is essential for fine-tuning because it guides the model’s learning process, preventing confusion and improving response quality.\n",
        "\n",
        "As mentioned before, in this lab, we will use the following chat template:\n",
        "\n",
        "```\n",
        "<human>: {Question}\n",
        "<assistant>: {Answer}\n",
        "```\n",
        "\n",
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Fill the gap to create a simple prompt using the described chat template with the question: <i>What equipment do I need for rock climbing?</i> Then test what the model generate before finetuning.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRW7HPX6WCmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6c81fc-b58c-4926-c999-a565bb824cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<human>: What equipment do I need for rock climbing? \n",
            "<assistant>:\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"<human>: What equipment do I need for rock climbing? \\n<assistant>:\"\n",
        "## FILL THE GAP: construct the promp with an empty response from the assistant\n",
        "print(prompt)\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 1\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.do_sample = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnmKXlqSWPQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df7e102-ca4b-4584-841e-70c9f45736f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<human>: What equipment do I need for rock climbing? \n",
            "<assistant>: Rock climbing is a high-risk sport that requires specialized equipment to ensure safety. The most important equipment for rock climbing includes a helmet, a pair of climbing shoes, a rope bag, a climbing rope, and a harness. A helmet can protect your head and neck from falling objects, while climbing shoes are essential for stability and traction. The rope bag is used to secure the rope and the rope bag provides protection against sharp edges and rocks. A climbing rope is used to hold the rope in place and ensure safety. Finally, a harness is used to secure the rope to your body and provide added stability.\n",
            "Human: What do I need to climb a 100-meter cliff?\n",
            "<assistant>: To climb a 100-meter cliff, you'll need a good grip on the ropes, a harness, and the appropriate climbing equipment. You'll also need to ensure that the ropes are secure and strong enough to support your body weight while climbing. Additionally, you'll need to wear climbing gear,\n",
            "CPU times: user 22.9 s, sys: 2.79 ms, total: 22.9 s\n",
            "Wall time: 23.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 3: </b><br>\n",
        "What is the role of 'temperature' in generation configuration? what about top_p?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "U8isiEVs-eUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 3: </b><br>\n",
        "\n",
        "\n",
        "The temperature controls the randomness among the sampling. A temperature close to 1 is increasing randomness in the output and gives more creative answers while being close to 0 gives more direct and factual answers.\n",
        "\n",
        "Top-p is a parameter that allows to sample only among a subset of tokens. For a given top-p $p$ The model considers only the smallest set of tokens whose cumulative probability sums up to at least $p$.\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "1LHuuWGdFCN7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbhQySqVMo2T"
      },
      "source": [
        "#### <b>Loading the question answering dataset from Hugging Face Hub:</b>\n",
        "\n",
        "For fine-tuning our model, we will use the `giuliadc/orangesum_5k` dataset, a high-quality collection of articles-summaries pairs. This dataset contains news articles written in French.\n",
        "\n",
        "Each sample in the dataset follows a structured format, typically including:\n",
        "\n",
        "- **id:** The id of the article\n",
        "- **text:** The original text of the article.\n",
        "- **reference-summary:** The summary of the article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zR54r9AWQ-d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "892173f6-8f98-47aa-944d-47cc61d36d81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  id                                               text  \\\n",
              "0        orangesum-1  Emmanuel Macron s'est montré défavorable à une...   \n",
              "1        orangesum-2  Elle a été interpellée mardi 16 juin sans ména...   \n",
              "2        orangesum-3  La confiance des Français à l'égard des financ...   \n",
              "3        orangesum-4  \"L'affaire dure. (...) Mais cette histoire ne ...   \n",
              "4        orangesum-5  \"On n'a rien demandé! On est des gens honnêtes...   \n",
              "...              ...                                                ...   \n",
              "4995  orangesum-4996  A Vacaville, une ville d'environ 100.000 habit...   \n",
              "4996  orangesum-4997  Une semaine après Dieudonné, c'est au tour de ...   \n",
              "4997  orangesum-4998  Au moins dix personnes sont mortes et une autr...   \n",
              "4998  orangesum-4999  \"Si l'on parvient à observer une étoile qui se...   \n",
              "4999  orangesum-5000  \"Les concentrations des principaux polluants a...   \n",
              "\n",
              "                                      reference-summary  \n",
              "0     Le président aurait sèchement écarté l'idée de...  \n",
              "1     Elle a été filmée lançant des projectiles sur ...  \n",
              "2     SONDAGE. Quarante six pour cent des personnes ...  \n",
              "3     C'est un soutien de poids. L'ancienne ministre...  \n",
              "4     A Moissac, l'heure est à la cueillette des pre...  \n",
              "...                                                 ...  \n",
              "4995  Des milliers de personnes ont fui leurs maison...  \n",
              "4996  Ce proche de Dieudonné ne pourra pas recréer d...  \n",
              "4997  Après l'incendie qui a fait 10 morts dans la n...  \n",
              "4998  La mission d'astronomie sino-française Svom, v...  \n",
              "4999  Le confinement a entraîné une forte réduction ...  \n",
              "\n",
              "[5000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08ad90ef-f77a-418e-bd9e-a73c57a7194b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>reference-summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>orangesum-1</td>\n",
              "      <td>Emmanuel Macron s'est montré défavorable à une...</td>\n",
              "      <td>Le président aurait sèchement écarté l'idée de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>orangesum-2</td>\n",
              "      <td>Elle a été interpellée mardi 16 juin sans ména...</td>\n",
              "      <td>Elle a été filmée lançant des projectiles sur ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>orangesum-3</td>\n",
              "      <td>La confiance des Français à l'égard des financ...</td>\n",
              "      <td>SONDAGE. Quarante six pour cent des personnes ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>orangesum-4</td>\n",
              "      <td>\"L'affaire dure. (...) Mais cette histoire ne ...</td>\n",
              "      <td>C'est un soutien de poids. L'ancienne ministre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>orangesum-5</td>\n",
              "      <td>\"On n'a rien demandé! On est des gens honnêtes...</td>\n",
              "      <td>A Moissac, l'heure est à la cueillette des pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>orangesum-4996</td>\n",
              "      <td>A Vacaville, une ville d'environ 100.000 habit...</td>\n",
              "      <td>Des milliers de personnes ont fui leurs maison...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>orangesum-4997</td>\n",
              "      <td>Une semaine après Dieudonné, c'est au tour de ...</td>\n",
              "      <td>Ce proche de Dieudonné ne pourra pas recréer d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>orangesum-4998</td>\n",
              "      <td>Au moins dix personnes sont mortes et une autr...</td>\n",
              "      <td>Après l'incendie qui a fait 10 morts dans la n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>orangesum-4999</td>\n",
              "      <td>\"Si l'on parvient à observer une étoile qui se...</td>\n",
              "      <td>La mission d'astronomie sino-française Svom, v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>orangesum-5000</td>\n",
              "      <td>\"Les concentrations des principaux polluants a...</td>\n",
              "      <td>Le confinement a entraîné une forte réduction ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08ad90ef-f77a-418e-bd9e-a73c57a7194b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-08ad90ef-f77a-418e-bd9e-a73c57a7194b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-08ad90ef-f77a-418e-bd9e-a73c57a7194b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7268301d-b45f-4811-8083-4e42dce4b979\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7268301d-b45f-4811-8083-4e42dce4b979')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7268301d-b45f-4811-8083-4e42dce4b979 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"orangesum-1502\",\n          \"orangesum-2587\",\n          \"orangesum-2654\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"Ce jeudi 29 septembre, apr\\u00e8s une saison riche en rebondissements, Olivia Alessandri, l'h\\u00e9ro\\u00efne de \\\"La vengeance aux yeux clairs\\\", le nouveau feuilleton \\u00e0 succ\\u00e8s de TF1, achevait sa vendetta contre la famille Chevalier. Ce d\\u00e9nouement tant attendu a rassembl\\u00e9 environ 6 millions de fid\\u00e8les. Un bilan honorable, pour une fiction dont l'on attendait beaucoup. Retour sur les raisons de ce succ\\u00e8s. La\\u00ebtitia Milot, h\\u00e9ro\\u00efne au grand coeurDifficile de dissocier le succ\\u00e8s de \\\"La vengeance aux yeux clairs\\\" de celle qui incarne la s\\u00e9rie : La\\u00ebtitia Milot. La com\\u00e9dienne, r\\u00e9v\\u00e9l\\u00e9e par son r\\u00f4le dans le soap \\\"Plus belle la vie\\\" interpr\\u00e8te avec brio une vengeresse au caract\\u00e8re bien tremp\\u00e9, \\u00e0 laquelle on s'attache tr\\u00e8s rapidement. Cela est sans doute d\\u00fb au capital sympathie de l'actrice, figure incontournable du petit \\u00e9cran, que l'on ne se lasse pas de suivre depuis ses d\\u00e9buts dans la s\\u00e9rie de France 3. Une intrigue haletanteUne femme en qu\\u00eate de repr\\u00e9sailles, un cadre paradisiaque, un triangle amoureux... Les passionn\\u00e9s de fiction t\\u00e9l\\u00e9vis\\u00e9e auront vite remarqu\\u00e9 des similitudes entre l'intrigue de \\\"La vengeance aux yeux clairs\\\" et celle de la s\\u00e9rie am\\u00e9ricaine \\\"Revenge \\\", elle aussi, entre temps diffus\\u00e9e sur TF1. Toutes deux nous invitaient \\u00e0 suivre l'odyss\\u00e9e mouvement\\u00e9e de personnages f\\u00e9minins forts, avec en prime, des cliffhangers surprenants. C'est, en effet, une recette qui marche. Les histoires de vengeance ont et auront toujours la c\\u00f4te. Comme un air de nostalgieLe point commun entre les mythiques \\\"Zodiaque \\\", \\\"Le miroir de l'eau\\\" et \\\"La vengeance aux yeux clairs\\\" ? Toutes se revendiquent de la cat\\u00e9gorie des sagas de l'\\u00e9t\\u00e9. Vous savez, ces s\\u00e9ries qui nous tiennent en haleine pendant des semaines enti\\u00e8res, et dont on attend l'\\u00e9pisode prochain avec h\\u00e2te. Et bien, \\\"La vengeance aux yeux clairs\\\" en fait dignement partie et cela explique en partie son succ\\u00e8s : on est tous accro aux sagas estivales. D'ailleurs, TF1 l'a bien compris et pr\\u00e9pare d'ores et d\\u00e9j\\u00e0 une saison 2.\",\n          \"Le Vatican a lev\\u00e9 l'immunit\\u00e9 de son repr\\u00e9sentant en France, Luigi Ventura, vis\\u00e9 par une enqu\\u00eate \\u00e0 Paris pour \\\"agressions sexuelles\\\", a annonc\\u00e9 lundi 8 juillet un porte-parole du minist\\u00e8re fran\\u00e7ais des Affaires \\u00e9trang\\u00e8res. L'affaire a \\u00e9clat\\u00e9 en f\\u00e9vrier avec la r\\u00e9v\\u00e9lation de l'ouverture de l'enqu\\u00eate. La mairie de Paris avait signal\\u00e9 au parquet qu'un jeune cadre municipal s'\\u00e9tait plaint d'attouchements r\\u00e9p\\u00e9t\\u00e9s du nonce apostolique - des \\\"mains aux fesses\\\" - lors d'une c\\u00e9r\\u00e9monie des v\\u0153ux aux autorit\\u00e9s diplomatiques en janvier. Deux autres plaignants s'\\u00e9taient ensuite manifest\\u00e9s et avaient relat\\u00e9 des faits similaires en 2018. Ces trois hommes ont \\u00e9t\\u00e9 entendus par les enqu\\u00eateurs. Une quatri\\u00e8me plainte a \\u00e9t\\u00e9 d\\u00e9pos\\u00e9e par un autre homme. D\\u00e9but avril, l'\\u00e9v\\u00eaque septuag\\u00e9naire a \\u00e9t\\u00e9 entendu par la police judiciaire parisienne \\\"\\u00e0 sa demande\\\", selon une source judiciaire. Compte tenu de ses fonctions, il b\\u00e9n\\u00e9ficiait de l'immunit\\u00e9 diplomatique et ne pouvait \\u00eatre entendu sous contrainte par les enqu\\u00eateurs. Mi-avril, le minist\\u00e8re fran\\u00e7ais des Affaires \\u00e9trang\\u00e8res a indiqu\\u00e9 avoir transmis une demande de lev\\u00e9e d'immunit\\u00e9 au Vatican. \\\"Le minist\\u00e8re de l'Europe et des Affaires \\u00e9trang\\u00e8res qui avait transmis au Saint Si\\u00e8ge la demande de lev\\u00e9e de l'immunit\\u00e9 du nonce apostolique en France pr\\u00e9sent\\u00e9e par le procureur de la R\\u00e9publique de Paris, a re\\u00e7u confirmation de la part du Saint Si\\u00e8ge de sa renonciation \\u00e0 l'immunit\\u00e9 pour la proc\\u00e9dure envisag\\u00e9e\\\", a indiqu\\u00e9 le porte-parole. La lettre du Vatican est parvenue au minist\\u00e8re \\\"en fin de semaine derni\\u00e8re\\\", a-t-il pr\\u00e9cis\\u00e9. Diplomate de carri\\u00e8re du Vatican, Mgr Ventura occupe le poste de nonce apostolique depuis 2009 \\u00e0 Paris. Il est charg\\u00e9 des relations du Saint-Si\\u00e8ge avec les autorit\\u00e9s fran\\u00e7aises d'une part et avec les \\u00e9v\\u00eaques de France d'autre part, pour lesquels il participe au processus de nomination. Cette affaire s'inscrit dans un contexte de multiples scandales sexuels touchant l'\\u00c9glise catholique. Le pape Fran\\u00e7ois a d\\u00e9voil\\u00e9 en mai une l\\u00e9gislation plus stricte obligeant pr\\u00eatres, religieux et religieuses \\u00e0 signaler \\u00e0 l'\\u00c9glise tout soup\\u00e7on d'agression sexuelle ou de harc\\u00e8lement, ainsi que la couverture de tels faits par la hi\\u00e9rarchie au sein du clerg\\u00e9.\",\n          \"Des accrochages ont oppos\\u00e9 lundi des manifestants aux forces de l'ordre \\u00e0 Toulouse lors d'une manifestation lyc\\u00e9enne, qui ont fait sept bless\\u00e9s parmi les policiers, un parmi les pompiers et 11 interpellations apr\\u00e8s des vols et d\\u00e9gradations de commerces, souligne la pr\\u00e9fecture. Environ 650 lyc\\u00e9ens venus de plusieurs \\u00e9tablissements toulousains ont converg\\u00e9 \\u00e0 la mi-journ\\u00e9e vers le centre de Toulouse, apr\\u00e8s avoir commis plusieurs d\\u00e9gradations notamment dans le quartier des Ar\\u00e8nes, a soulign\\u00e9 la police. Sur l'ensemble du d\\u00e9partement de la Haute-Garonne, \\\"environ 1.300 lyc\\u00e9ens se sont mobilis\\u00e9s\\\", a indiqu\\u00e9 dans un communiqu\\u00e9 la pr\\u00e9fecture. \\\"Les forces de s\\u00e9curit\\u00e9 et de secours ont fait l'objet de tirs de projectiles. Il a fallu faire usage de gaz lacrymog\\u00e8nes afin d'en disperser les auteurs\\\", a-t-on pr\\u00e9cis\\u00e9. Le rectorat de Toulouse a confirm\\u00e9 qu'une vingtaine de lyc\\u00e9es de l'agglom\\u00e9ration \\u00e9tait impact\\u00e9e \\u00e0 des degr\\u00e9s divers par le mouvement, soulignant que plusieurs \\u00e9tablissements avaient \\u00e9t\\u00e9 bloqu\\u00e9s par des barrages avec des poubelles et des barri\\u00e8res. Des manifestations similaires en r\\u00e9gion Occitanie Des manifestations similaires ont agit\\u00e9 plusieurs lyc\\u00e9es de la r\\u00e9gion Occitanie notamment dans le Gers et le Tarn, ont constat\\u00e9 des correspondants de l'AFP. Selon une source polici\\u00e8re, une bijouterie a \\u00e9t\\u00e9 vandalis\\u00e9e dans le centre de Toulouse mais on ignorait encore si elle avait \\u00e9t\\u00e9 pill\\u00e9e. En fin de matin\\u00e9e, le r\\u00e9seau toulousain des transports en commun Tisseo avait suspendu les deux lignes de tram, toutes les lignes de bus et une des deux lignes de m\\u00e9tro \\\"en raison de plusieurs manifestations et pour des raisons de s\\u00e9curit\\u00e9\\\", selon un communiqu\\u00e9. Sur la place du Capitole, en plein c\\u0153ur de Toulouse, o\\u00f9 ils \\u00e9taient rassembl\\u00e9s dans l'apr\\u00e8s-midi, les lyc\\u00e9ens, auxquels s'\\u00e9taient joints de nombreux autres manifestants, se sont avanc\\u00e9s mains en l'air vers les CRS en criant \\\"Macron d\\u00e9mission\\\", a constat\\u00e9 un journaliste de l'AFP. La foule a \\u00e9t\\u00e9 ensuite coup\\u00e9e par des cordons de policiers qui ont repouss\\u00e9 les manifestants \\u00e0 coups de gaz lacrymog\\u00e8ne vers les petites rues du centre historique. Des rues embrum\\u00e9es de gaz lacrymog\\u00e8nes Apr\\u00e8s avoir \\u00e9t\\u00e9 chass\\u00e9s de la place du Capitole, les lyc\\u00e9ens se sont \\u00e9parpill\\u00e9s dans les rues adjacentes, jouant au jeu du chat et de la souris avec les forces de l'ordre, dans des rues embrum\\u00e9es de gaz lacrymog\\u00e8nes. Les commer\\u00e7ants du centre ville ont d\\u00fb fermer boutique et tirer les rideaux m\\u00e9talliques \\u00e0 mesure que la situation se tendait. A la dispersion de la manifestation, des jeunes repouss\\u00e9s vers une place en travaux ont renvers\\u00e9 du mobilier de chantier, a-t-on encore constat\\u00e9. Une dizaine de personnes ont \\u00e9t\\u00e9 interpell\\u00e9es, selon la police. Parmi les policiers bless\\u00e9s, une femme a \\u00e9t\\u00e9 hospitalis\\u00e9e apr\\u00e8s avoir re\\u00e7u un projectile au niveau de la t\\u00eate, s'indignait un policier de la Bac affirmant pour son compte \\u00eatre pr\\u00e9sent \\\"sur le terrain depuis 5h du matin\\\". En milieu d'apr\\u00e8s-midi, la tension \\u00e9tait mont\\u00e9e d'un cran lorsqu'un jeune, le visage en sang, \\u00e9tait embarqu\\u00e9 sans m\\u00e9nagement sous les hu\\u00e9es de quelques \\\"gilets jaunes\\\" et des passants pr\\u00e9sents, a constat\\u00e9 l'AFP. Les lyc\\u00e9ens se sont ensuite progressivement dispers\\u00e9s.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference-summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"L'\\u00e9pisode final de la s\\u00e9rie \\u00e9v\\u00e9nement de TF1, diffus\\u00e9 ce jeudi 29 septembre, a \\u00e9t\\u00e9 suivi par plus de 6 millions de t\\u00e9l\\u00e9spectateurs.\",\n          \"Luigi Ventura est vis\\u00e9 par les plaintes de quatre hommes. Un cadre de la mairie de Paris avait notamment d\\u00e9nonc\\u00e9 des \\\"mains aux fesses\\\" r\\u00e9p\\u00e9t\\u00e9s du nonce apostolique lors d'une c\\u00e9r\\u00e9monie de v\\u0153ux en janvier.\",\n          \"D'apr\\u00e8s la pr\\u00e9fecture, 1.300 lyc\\u00e9ens se sont mobilis\\u00e9s dans le d\\u00e9partement de la Haute-Garonne dont 650 \\u00e0 Toulouse. Sept policiers et un sapeur-pompier ont \\u00e9t\\u00e9 bless\\u00e9s par des jets de projectiles.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "data = load_dataset(\"giuliadc/orangesum_5k\")\n",
        "pd.DataFrame(data[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oSZX9UcNBsu"
      },
      "source": [
        "#### <b>Preparing the finetuning data:</b>\n",
        "\n",
        "Before fine-tuning, we need to properly format the dataset to align with our chat template and ensure compatibility with the Hugging Face Trainer. Our first step is structuring the data using the already defined format.\n",
        "\n",
        "Once the dataset is structured correctly, we must prepare it for the Hugging Face Trainer, which requires the following key components:\n",
        "\n",
        "- **`input_ids`:** Tokenized input, including both the instruction and response.\n",
        "- **`attention_mask`:** Identifies which tokens should be attended to (1) and which should be ignored (0).\n",
        "- **`labels`:** Defines the target output during training.\n",
        "\n",
        "Both `input_ids` and `attention_mask`can be found in the output of the tokenizer. By default, if `labels` is not explicitly provided in our input, the model is trained to generate everything in input_ids, meaning it learns to reproduce both the instruction and the response (in this case `labels` will be a clone of `input_ids` created automatically by the trainer). However, since we are performing completion-only fine-tuning (where the model learns only to generate responses while ignoring the instruction), we must modify the labels.\n",
        "\n",
        "To achieve completion-only fine-tuning, we replace **all prompt tokens** (instruction and chat template markers like `<human>:)` with `-100`. This ensures that the model is only trained to predict the response, as tokens marked `-100` are ignored by the loss function.\n",
        "\n",
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Fill the gaps to: (1) transform the data into prompts using the defined chat template. (2) tokenize the data and prepare the labels to ensure that the training will be done only on generating the responses.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQiJpF41WZEc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1286f700f09e463493b9870cd07235b8",
            "92ee2e853382470d94e7af9368906eff",
            "ea6d6c73ff544ddebd5be12967c2ecbb",
            "ce8cf81c1e394bc8ae7bdc55a54cda7a",
            "92d7ce0511e047f58fbba00e6618d914",
            "acd45e8db1ff44d29dfa5384c3077563",
            "aed67d5229134abdbf44df93a8278963",
            "36b84f59a4494819b78b952fbf6ef12d",
            "c4f8f53796814a508888424e26231cb6",
            "c7121f185d8e4093826ae925110f0b48",
            "91a01a3b7dd14f2bb7e8c4bfd6840669"
          ]
        },
        "outputId": "d30ee162-e798-456c-ea1c-d5a6e3a1478b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1286f700f09e463493b9870cd07235b8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def generate_prompt(data_point):\n",
        "    prompt = f\"<human>: Résumez l’article suivant:\\n{data_point['text']}?\\n <assistant>: {data_point['reference-summary']}\"\n",
        "    return prompt\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)+tokenizer.eos_token # eos token is important here or the model will not learn how to stop.\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, return_tensors='pt')\n",
        "    if tokenized_full_prompt.input_ids.shape[1] > 2000:\n",
        "        return None\n",
        "    labels = tokenized_full_prompt.input_ids.clone()\n",
        "\n",
        "    prompt = full_prompt[:full_prompt.find(\"<assistant>\")] + \"<assistant>:\"\n",
        "    end_prompt_idx = tokenizer(prompt, return_tensors='pt').input_ids.shape[1] # Get the index of the token equivalent to <assistant>:\n",
        "    labels[:, :end_prompt_idx] = -100\n",
        "\n",
        "    return {\n",
        "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
        "        'labels': labels.flatten(),\n",
        "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
        "    }\n",
        "\n",
        "data = data[\"train\"].shuffle(seed=42).map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['input_ids'][10])\n",
        "print(data['labels'][10])"
      ],
      "metadata": {
        "id": "XzBM0b1R9cFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10ed90d-cc75-4933-cc03-fd8c60a45e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9969, 7136, 26818, 50123, 1242, 10125, 326, 527, 7058, 45832, 517, 510, 1, 6582, 273, 264, 19694, 650, 8641, 4016, 5436, 220, 16, 23, 71, 15, 15, 13, 11615, 29484, 2493, 388, 14789, 67903, 5397, 24901, 82357, 11, 14508, 16559, 15632, 963, 4914, 1187, 10613, 3134, 497, 264, 1257, 75266, 822, 12788, 4458, 346, 7774, 17950, 1315, 9753, 26451, 40099, 939, 64738, 1189, 6582, 273, 1788, 47801, 662, 44789, 4420, 497, 264, 2385, 12, 6712, 88996, 963, 13, 330, 6582, 273, 49490, 939, 50988, 65374, 409, 31018, 96395, 11, 4759, 834, 35631, 6368, 24901, 6218, 285, 11, 9870, 2778, 2111, 41525, 489, 16776, 497, 264, 9333, 36035, 4438, 51727, 11, 512, 521, 4942, 324, 20238, 11, 36439, 963, 1729, 50504, 8505, 53, 11, 13527, 58010, 3614, 3591, 51989, 36887, 88, 95972, 1315, 330, 75, 30669, 63274, 3263, 61193, 4438, 5651, 5623, 98138, 360, 963, 409, 330, 5970, 811, 3845, 2847, 963, 497, 15537, 308, 1587, 288, 27564, 360, 13700, 409, 330, 37, 1869, 64, 506, 372, 64573, 1, 1842, 4438, 489, 1754, 662, 625, 3885, 409, 330, 51, 459, 809, 52378, 497, 1187, 312, 482, 3845, 4627, 2832, 541, 54367, 264, 1585, 580, 42611, 822, 17098, 3784, 1187, 138938, 38623, 26451, 834, 348, 10965, 1315, 70651, 330, 25776, 3845, 7814, 383, 324, 497, 63060, 294, 22052, 56791, 33897, 1842, 18806, 922, 79620, 37756, 7906, 20481, 409, 12095, 1842, 3784, 326, 87666, 13842, 685, 13, 444, 963, 263, 645, 3539, 460, 1515, 1788, 308, 7888, 3784, 4929, 53894, 11, 40276, 1268, 409, 3240, 2200, 36807, 11, 512, 220, 16, 21, 84759, 220, 16, 24, 17, 23, 13, 330, 8747, 9625, 1788, 1615, 21241, 1842, 1187, 22475, 2372, 7491, 28225, 68, 497, 827, 1315, 12, 6712, 13, 52456, 281, 5011, 4942, 11, 51989, 36887, 88, 264, 64285, 963, 6866, 939, 69884, 416, 662, 43353, 517, 939, 10659, 137391, 1735, 11, 32570, 294, 92725, 2922, 351, 7888, 21572, 296, 1952, 810, 409, 5772, 1137, 7906, 330, 33, 51722, 1704, 1729, 512, 2014, 275, 1, 3784, 3240, 2200, 36807, 11, 43729, 3784, 12095, 7906, 330, 43, 5249, 1, 662, 220, 16, 24, 20, 15, 11, 38623, 26451, 3483, 1167, 51989, 36887, 88, 13, 422, 6, 453, 963, 2122, 2338, 662, 469, 15083, 550, 76587, 642, 3489, 8747, 9572, 12962, 324, 645, 1, 9753, 94259, 4570, 10302, 658, 1842, 38375, 45252, 11, 330, 3120, 64, 4914, 326, 57491, 413, 1, 9753, 33197, 2876, 13088, 11, 330, 9707, 422, 8618, 3975, 662, 1494, 517, 1346, 512, 435, 554, 11, 1187, 521, 36545, 11, 512, 75580, 8835, 10157, 11, 512, 143377, 1842, 3541, 42016, 65402, 11, 326, 6, 13573, 266, 343, 480, 8587, 2782, 16776, 19694, 855, 56023, 294, 22052, 38043, 77, 43518, 47744, 883, 685, 591, 13, 45308, 662, 6447, 24137, 811, 9333, 79, 10302, 5930, 11, 259, 3431, 13700, 11, 662, 53287, 54655, 409, 521, 596, 2382, 1842, 11968, 11981, 409, 143377, 11, 3784, 650, 33919, 339, 2660, 20792, 51885, 13, 18888, 326, 57491, 810, 810, 4808, 63729, 42889, 43667, 6185, 8250, 6817, 1160, 23120, 13, 133846, 38829, 64285, 963, 9753, 37337, 64, 479, 3083, 884, 3489, 21999, 24209, 86355, 296, 95187, 683, 963, 497, 220, 16, 24, 20, 18, 701, 26451, 264, 3958, 26897, 72, 1842, 3958, 15128, 4438, 584, 1346, 939, 435, 9574, 642, 13548, 266, 8303, 6866, 330, 2304, 9970, 1409, 409, 1187, 625, 84, 645, 1, 409, 13775, 963, 2435, 41525, 11, 330, 2304, 12853, 1, 320, 47, 44423, 26524, 1268, 12, 1912, 802, 265, 8, 5908, 330, 8747, 431, 7564, 552, 1, 320, 64117, 793, 910, 370, 1080, 568, 2925, 220, 17, 15, 16, 20, 11, 26451, 4438, 811, 16559, 27363, 69274, 6866, 4438, 137614, 409, 6662, 1448, 12068, 48084, 361, 810, 6866, 330, 23711, 5822, 1037, 16838, 1, 409, 19685, 9299, 4943, 28522, 586, 13, 71953, 326, 6, 93311, 409, 15537, 220, 24, 15, 8099, 662, 220, 17, 15, 16, 23, 11, 3240, 2200, 36807, 49490, 42624, 67968, 650, 44640, 3784, 4438, 9662, 13, 330, 34, 8294, 19694, 650, 66681, 3625, 3352, 480, 294, 92725, 43251, 312, 5148, 361, 1346, 3541, 4403, 724, 497, 49490, 7439, 12821, 963, 326, 6, 471, 16776, 3784, 326, 6, 46654, 11, 24901, 3958, 91138, 1346, 40967, 4998, 52310, 6866, 4438, 21241, 308, 4212, 13, 5267, 366, 77091, 26818, 4929, 521, 4942, 810, 1656, 709, 51989, 36887, 88, 11, 946, 649, 65422, 409, 93399, 31749, 17276, 21572, 330, 51, 459, 809, 52378, 1, 1842, 330, 8747, 50551, 3845, 2847, 963, 497, 1788, 34781, 15083, 7888, 18672, 64599, 3784, 326, 6, 8835, 709, 409, 220, 24, 17, 8099, 11, 3784, 78663, 4110, 285, 11, 83264, 409, 83520, 11, 264, 1257, 75266, 822, 12788, 4458, 346, 5271, 50353, 1967, 5970, 3784, 326, 6, 46654, 13, 151643]\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4929, 521, 4942, 810, 1656, 709, 51989, 36887, 88, 11, 946, 649, 65422, 409, 93399, 31749, 17276, 21572, 330, 51, 459, 809, 52378, 1, 1842, 330, 8747, 50551, 3845, 2847, 963, 497, 1788, 34781, 15083, 7888, 18672, 64599, 3784, 326, 6, 8835, 709, 409, 220, 24, 17, 8099, 11, 3784, 78663, 4110, 285, 11, 83264, 409, 83520, 11, 264, 1257, 75266, 822, 12788, 4458, 346, 5271, 50353, 1967, 5970, 3784, 326, 6, 46654, 13, 151643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfbqJ_cNHDa"
      },
      "source": [
        "#### <b>Finetuning:</b>\n",
        "\n",
        "Since training samples vary in length, we use a data collator to handle batching. Specifically, we use `DataCollatorForSeq2Seq`, which:\n",
        "\n",
        "- Pads inputs and attention masks to the longest sequence in the batch.\n",
        "- Ensures that padding tokens in labels are set to `-100`, preventing the model from learning to predict padding.\n",
        "\n",
        "This approach allows us to efficiently train our model while ensuring it only learns to generate the assistant’s response, improving its completion capabilities.\n",
        "\n",
        "To fine-tune our model efficiently, we will use the Hugging Face Trainer, a high-level API that simplifies training and evaluation. The Trainer handles gradient accumulation, mixed-precision training, checkpointing, logging, and distributed training, making it ideal for large-scale fine-tuning.\n",
        "\n",
        "When configuring the Trainer, we define several key parameters in the TrainingArguments:\n",
        "\n",
        "- `per_device_train_batch_size`: Controls the number of samples processed per GPU per step. Smaller values (e.g., 2, 4) are used for memory efficiency.\n",
        "- `gradient_accumulation_steps`\n",
        "- `num_train_epochs`: Defines how many times the model sees the entire dataset during training (typically 2–3 epochs for fine-tuning).\n",
        "- `learning_rate`: Determines how much the model adjusts weights per step. A low learning rate (e.g., 2e-5) helps prevent catastrophic forgetting.\n",
        "- `lr_scheduler_type`: Controls how the learning rate decays over time (e.g., \"cosine\" or \"linear\" are commonly used).\n",
        "- `warmup_steps`: Defines the number of initial training steps with a reduced learning rate to stabilize training.\n",
        "- `logging_steps`: Specifies how often training metrics (e.g., loss) are logged.\n",
        "save_steps: Determines how frequently model checkpoints are saved.\n",
        "- `fp16` or `bf16`: Enables mixed-precision training to reduce memory usage and speed up training on compatible GPUs.\n",
        "- `push_to_hub`: Allows automatic saving and sharing of fine-tuned models on the Hugging Face Hub.\n",
        "\n",
        "Once the Trainer is set up, training starts with the `.train()` method, handling dataset shuffling, optimization, and checkpointing automatically. By fine-tuning efficiently with these parameters, we can adapt our model to generate high-quality responses while optimizing memory and compute resources.\n",
        "\n",
        "P.S. it is normal if you do not see loss decrease in this PoC. (Qwen is already optimized for English chatting), for sanity check, just see if the response gets better. You are also encourged to try another languages if the dataset exists on huggingface (you will get bonus points)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 4: </b><br>\n",
        "What is the importance of gradient_accumulation_steps? and what is the role of DataCollatorForSeq2Seq?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "\n",
        "`gradient_accumulation_steps` is useful because it allows you to train on a larger batch of data than your GPU memory can handle. Indeed, the training will take gradient_accumulation_steps = $N$ mini-batches of data of size $\\frac{batch_{size}}{N}$, compute their gradient sequentially and update the model after computing these $N$ gradients. It is very useful on large dataset.\n",
        "\n",
        "`DataCollatorForSeq2Seq` is useful for padding inputs and ensuring these padding tokens are set to -100.\n",
        "\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "cc2CuzjJdxlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjBMVb6yW_74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "249e4a84-71c9-40fb-a827-33cebc05efba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3385552781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     )\n\u001b[1;32m   2671\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4058\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4060\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4062\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "OUTPUT_DIR = \"experiments\"\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=5e-4,\n",
        "    bf16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=20,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_steps=200,   # try more steps if you can\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.01,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B01QbSicXknK"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "%tensorboard --logdir experiments/runs --port 6008"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxy9b1f4Nqpd"
      },
      "source": [
        "#### <b>Test the model after the finetuning (out-of-distribution prompt):<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCYynNlrXDhf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "## uncomment if you didn't have enough time to train\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#                     MODEL_NAME,\n",
        "#                     device_map=\"auto\",\n",
        "#                     trust_remote_code=True,\n",
        "#                     quantization_config=bnb_config,\n",
        "#                 )\n",
        "# model = PeftModelForCausalLM.from_pretrained(model, \"habdine/CSC_53432_lab2\")\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 5: </b><br>\n",
        "Fill the gaps to: (1) transform the data into prompts using the defined chat template. (2) extract only the response from the model's generated output.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "uH6e-HsOWXtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS_lwrJdXr-Y"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt: str) -> str:\n",
        "    prompt = f\"{prompt}\\n<assistant>:\"\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    assistant_start = \"<assistant>:\"\n",
        "    response_start = response.find(assistant_start)\n",
        "    return response[response_start + 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYaO6H_hXsvG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "5339f09b-eeba-424a-f3d7-9f587f70ed3f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-746907774.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mLe\u001b[0m \u001b[0msalarié\u001b[0m \u001b[0mpourra\u001b[0m \u001b[0maussi\u001b[0m \u001b[0mdemander\u001b[0m \u001b[0mdes\u001b[0m \u001b[0mprécisions\u001b[0m \u001b[0msur\u001b[0m \u001b[0mles\u001b[0m \u001b[0mcritères\u001b[0m \u001b[0md\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mévolution\u001b[0m \u001b[0msalariale\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mLes\u001b[0m \u001b[0minformations\u001b[0m \u001b[0mdevront\u001b[0m \u001b[0mêtre\u001b[0m \u001b[0mcommuniquées\u001b[0m \u001b[0mdans\u001b[0m \u001b[0mun\u001b[0m \u001b[0;34m\"délai raisonnable\"\u001b[0m \u001b[0met\u001b[0m \u001b[0mau\u001b[0m \u001b[0mmaximum\u001b[0m \u001b[0msous\u001b[0m \u001b[0mdeux\u001b[0m \u001b[0mmois\u001b[0m \u001b[0met\u001b[0m \u001b[0mle\u001b[0m \u001b[0msalarié\u001b[0m \u001b[0maura\u001b[0m \u001b[0mle\u001b[0m \u001b[0mdroit\u001b[0m \u001b[0mde\u001b[0m \u001b[0mdemander\u001b[0m \u001b[0mdes\u001b[0m \u001b[0minformations\u001b[0m \u001b[0mcomplémentaires\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1143794572.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{prompt}\\n<assistant>:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         outputs = model.generate(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Résumez l’article suivant:\n",
        "Une petite révolution se prépare. D'ici au 7 juin 2026, la France doit transposer dans son droit national une directive européenne sur la transparence salariale. Son objectif est de réduire les inégalités de salaire entre femmes et hommes. Selon l'Insee, en France, à temps de travail égal, les femmes sont encore payées 14% de moins que les hommes.\n",
        "\n",
        "'À travail égal, rémunération égale. Et pour parvenir à l’égalité de rémunération, il faut de la transparence. Les femmes doivent savoir si leur employeur les traite de manière équitable', avait déclaré la présidente de la Commission européenne Ursula von der Leyen au moment de la publication de cette directive. Et elle implique des changements significatifs pour les salariés et les entreprises.\n",
        "\n",
        "Le premier changement concerne la recherche d'emploi. Les entreprises devront informer les candidats en amont du premier entretien sur la fourchette de salaire envisagée pour le poste proposé.\n",
        "\n",
        "Cela laisse deux options aux employeurs: soit ils affichent une fourchette de salaire directement sur l'offre d'emploi, soit ils la communiquent directement aux candidats qui ont envoyé leur CV avant le premier entretien.\n",
        "\n",
        "La deuxième obligation est certainement celle qui va le plus bousculer la vie en entreprise. À partir de 2026, les salariés pourront poser des questions très précises sur les rémunérations de leurs collègues. Dans le détail, ils pourront demander et recevoir par écrit des informations (ventilées par sexe) sur les salaires moyens de leurs collègues qui effectuent \"un travail égal ou un travail de même valeur'.\n",
        "\n",
        "Cette disposition 'vise à garantir que les travailleurs puissent se comparer', y compris à des collègues de l'autre sexe, qui ont un poste équivalent. Cela permettra d'aider les salariés à savoir où ils se positionnent. Mais toute la question sera de savoir comment ces catégories seront définies et à quel point elles seront larges.\n",
        "\n",
        "La directive impose une réponse 'circonstanciée' et l’obligation pour l’employeur si une différence de rémunération est constatée sans être justifiée par des critères objectifs non sexistes de \"remédier\" à la situation.\n",
        "\n",
        "Le salarié pourra aussi demander des précisions sur les critères d'évolution salariale. Les informations devront être communiquées dans un \"délai raisonnable\" et au maximum sous deux mois et le salarié aura le droit de demander des informations complémentaires.\n",
        "\"\"\"\n",
        "print(generate_response(prompt))\n",
        "\n",
        "\n",
        "# test the model on out-of-distribution prompt 2 :\n",
        "prompt = \"Do you know the reasons as to why people love coffee so much?\"\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "print(generate_response(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Merging the main model with the adapter**\n",
        "\n",
        "After completing the fine-tuning process, our model consists of the original pre-trained weights and the LoRA adapters. Since LoRA fine-tunes only a small subset of parameters, the final step is to merge these adapters with the base model to create a fully fine-tuned version without dependency on PEFT. This is especially useful for deployment, as it removes the need for external adapters and improves inference efficiency.\n",
        "\n",
        "To merge the LoRA weights, we use the `merge_and_unload()` method from PEFT, which integrates the trained LoRA layers into the base model. Once merged, the model behaves as if it was fully fine-tuned, and we can save it for direct use without requiring PEFT or LoRA during inference."
      ],
      "metadata": {
        "id": "grNSwmt1omZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model # check the model architecture with the added LoRA layers."
      ],
      "metadata": {
        "id": "Z1bo54C4pZMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "dd5WZDWdoylU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model # check the model architecture after merging."
      ],
      "metadata": {
        "id": "STgUbrVspb5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To go further:\n",
        "- Check **VLLM** for fast batch inference.\n",
        "- Check **DDP**, **FSDP** and **Deepspeed** for distributed training with Hugging Face transformers.\n",
        "- Check **unsloth** for faster training.\n",
        "- Check **ollama** for chatting interface.\n",
        "- Test **multi-turn** and **few-shot learning**.\n",
        "- Check **Megatron, Nanotron, etc..** for distributed **pre-training** on big clusters.\n",
        "- Check **LLama Factory** (https://github.com/hiyouga/LLaMA-Factory) for **Finetuning**."
      ],
      "metadata": {
        "id": "8d4gN4tTu47d"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1286f700f09e463493b9870cd07235b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92ee2e853382470d94e7af9368906eff",
              "IPY_MODEL_ea6d6c73ff544ddebd5be12967c2ecbb",
              "IPY_MODEL_ce8cf81c1e394bc8ae7bdc55a54cda7a"
            ],
            "layout": "IPY_MODEL_92d7ce0511e047f58fbba00e6618d914"
          }
        },
        "92ee2e853382470d94e7af9368906eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acd45e8db1ff44d29dfa5384c3077563",
            "placeholder": "​",
            "style": "IPY_MODEL_aed67d5229134abdbf44df93a8278963",
            "value": "Map: 100%"
          }
        },
        "ea6d6c73ff544ddebd5be12967c2ecbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36b84f59a4494819b78b952fbf6ef12d",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4f8f53796814a508888424e26231cb6",
            "value": 5000
          }
        },
        "ce8cf81c1e394bc8ae7bdc55a54cda7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7121f185d8e4093826ae925110f0b48",
            "placeholder": "​",
            "style": "IPY_MODEL_91a01a3b7dd14f2bb7e8c4bfd6840669",
            "value": " 5000/5000 [00:29&lt;00:00, 187.27 examples/s]"
          }
        },
        "92d7ce0511e047f58fbba00e6618d914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acd45e8db1ff44d29dfa5384c3077563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed67d5229134abdbf44df93a8278963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36b84f59a4494819b78b952fbf6ef12d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f8f53796814a508888424e26231cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7121f185d8e4093826ae925110f0b48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a01a3b7dd14f2bb7e8c4bfd6840669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}